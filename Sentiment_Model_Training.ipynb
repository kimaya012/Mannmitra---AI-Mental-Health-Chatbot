{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de36eb39",
   "metadata": {},
   "source": [
    "# Sentiment Model Training for Mental Health Chatbot\n",
    "    \n",
    "This Jupyter Notebook guides you through the process of training a sentiment analysis model using a Kaggle dataset. The trained model will then be used by the chatbot to understand user emotions.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126a7386",
   "metadata": {},
   "source": [
    "## 1. Setup and Library Installation\n",
    "    First, ensure you have the necessary libraries installed. If you encounter errors, run the `pip install` commands in your terminal or directly in a new notebook cell prefixed with `!` (e.g., `!pip install pandas`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "979bc15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import joblib # For saving and loading the model and vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc70400",
   "metadata": {},
   "source": [
    " ## 2. Download NLTK Data\n",
    "    \n",
    "    NLTK (Natural Language Toolkit) requires some data files for tokenization and stopwords. Run this cell to download them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ab26bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK data checked/downloaded.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt')\n",
    "\n",
    "print(\"NLTK data checked/downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa7888a",
   "metadata": {},
   "source": [
    "## 3. Load the Dataset\n",
    "    Make sure you have downloaded `Sentiment_Analysis_for_Mental_Health.csv` from Kaggle and placed it in the `data/` folder inside your `chatbot_project` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09f3531f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully. Head of the dataframe:\n",
      "   Unnamed: 0                                          statement   status\n",
      "0           0                                         oh my gosh  Anxiety\n",
      "1           1  trouble sleeping, confused mind, restless hear...  Anxiety\n",
      "2           2  All wrong, back off dear, forward doubt. Stay ...  Anxiety\n",
      "3           3  I've shifted my focus to something else but I'...  Anxiety\n",
      "4           4  I'm restless and restless, it's been a month n...  Anxiety\n",
      "\n",
      "--- IMPORTANT: Columns in the dataset (check these carefully!) ---\n",
      "['Unnamed: 0', 'statement', 'status']\n",
      "-------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATASET_PATH = 'data/Sentiment_Analysis_for_Mental_Health.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(DATASET_PATH)\n",
    "    df.columns = df.columns.str.strip() # Good practice to strip whitespace\n",
    "\n",
    "    print(\"Dataset loaded successfully. Head of the dataframe:\")\n",
    "    print(df.head())\n",
    "\n",
    "    print(\"\\n--- IMPORTANT: Columns in the dataset (check these carefully!) ---\")\n",
    "    print(df.columns.tolist()) # <<< LOOK AT THIS OUTPUT!\n",
    "    print(\"-------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Dataset not found at '{DATASET_PATH}'. Please ensure it's in the 'data/' folder.\")\n",
    "    df = None\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred while loading the dataset: {e}\")\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cd9d2e",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing\n",
    "    We'll clean the text and map the mental health status labels to standard sentiment categories (positive, negative, neutral).\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335cd201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Filtered out 4078 rows due to unmapped 'label_original' values.\n",
      "Remaining rows: 48965 rows.\n",
      "\n",
      "Data after preprocessing and sentiment mapping:\n",
      "                                                text label_original  \\\n",
      "0                                         oh my gosh        Anxiety   \n",
      "1  trouble sleeping, confused mind, restless hear...        Anxiety   \n",
      "2  All wrong, back off dear, forward doubt. Stay ...        Anxiety   \n",
      "3  I've shifted my focus to something else but I'...        Anxiety   \n",
      "4  I'm restless and restless, it's been a month n...        Anxiety   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0                                            oh gosh  \n",
      "1  trouble sleeping confused mind restless heart ...  \n",
      "2  wrong back dear forward doubt stay restless re...  \n",
      "3  ive shifted focus something else im still worried  \n",
      "4                im restless restless month boy mean  \n",
      "\n",
      "Sentiment distribution (after mapping):\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'sentiment'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'sentiment'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 74\u001b[39m\n\u001b[32m     72\u001b[39m     \u001b[38;5;28mprint\u001b[39m(df.head())\n\u001b[32m     73\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSentiment distribution (after mapping):\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msentiment\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.value_counts())\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSkipping preprocessing: DataFrame is empty or not loaded correctly.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'sentiment'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# --- Configuration ---\n",
    "DATASET_PATH = '../data/Sentiment_Analysis_for_Mental_Health.csv'\n",
    "\n",
    "# --- NLTK Data Download (ensure these are downloaded if running standalone) ---\n",
    "# If you run this as a standalone script, you might need to uncomment and run these\n",
    "# lines once to ensure NLTK data is available.\n",
    "# try:\n",
    "#     nltk.data.find('corpora/stopwords')\n",
    "#     nltk.data.find('tokenizers/punkt')\n",
    "#     nltk.data.find('sentiment/vader_lexicon')\n",
    "# except nltk.downloader.DownloadError:\n",
    "#     print(\"Downloading NLTK data... This might take a moment.\")\n",
    "#     nltk.download('stopwords')\n",
    "#     nltk.download('punkt')\n",
    "#     nltk.download('vader_lexicon')\n",
    "# print(\"NLTK data checked/downloaded.\")\n",
    "\n",
    "\n",
    "# --- Section 3: Load the Dataset ---\n",
    "print(\"--- Starting Data Loading ---\")\n",
    "df = None # Initialize df to None for error handling\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(DATASET_PATH)\n",
    "    df.columns = df.columns.str.strip() # Strip whitespace from column names\n",
    "\n",
    "    print(\"Dataset loaded successfully. Head of the dataframe:\")\n",
    "    print(df.head())\n",
    "\n",
    "    print(\"\\n--- IMPORTANT: Columns in the dataset (verify these!) ---\")\n",
    "    print(df.columns.tolist())\n",
    "    print(\"----------------------------------------------------------\")\n",
    "\n",
    "    # This diagnostic block will check if 'status' column is present\n",
    "    if 'status' in df.columns:\n",
    "        print(\"\\nValue counts for 'status' column:\")\n",
    "        print(df['status'].value_counts())\n",
    "    else:\n",
    "        print(\"\\n'status' column not found. Please ensure your CSV has a column named 'status' (lowercase) or update the code accordingly.\")\n",
    "        df = None # Set df to None to prevent further errors if crucial column is missing\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Dataset not found at '{DATASET_PATH}'. Please ensure it's in the 'data/' folder.\")\n",
    "    print(\"Please download the dataset from: https://www.kaggle.com/datasets/suchintikasarkar/sentiment-analysis-for-mental-health\")\n",
    "    df = None\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred while loading the dataset: {e}\")\n",
    "    df = None\n",
    "\n",
    "\n",
    "# --- Section 4: Data Preprocessing ---\n",
    "print(\"\\n--- Starting Data Preprocessing ---\")\n",
    "\n",
    "if df is not None and not df.empty:\n",
    "    try:\n",
    "        # SELECTING THE CORRECT COLUMNS:\n",
    "        # Based on your output: ['Unnamed: 0', 'statement', 'status']\n",
    "        # We select 'statement' for text and 'status' for the original label.\n",
    "        df = df[['statement', 'status']].copy()\n",
    "        df.columns = ['text', 'label_original'] # Standardizing internal column names\n",
    "\n",
    "        # --- TEMPORARY DEBUGGING LINE (UNCOMMENT TO CHECK UNIQUE LABELS) ---\n",
    "        # print(\"\\nUnique values found in 'label_original' (your 'status' column):\")\n",
    "        # print(df['label_original'].unique())\n",
    "        # print(\"------------------------------------------------------------\")\n",
    "        # --- END OF TEMPORARY DEBUGGING LINE ---\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Missing expected column in dataset for preprocessing: {e}.\")\n",
    "        print(\"Please ensure your CSV has 'statement' and 'status' columns (case-sensitive) or update the selection: df[['your_text_column', 'your_status_column']].copy()\")\n",
    "        df = None\n",
    "\n",
    "if df is not None and not df.empty:\n",
    "    # LABEL MAPPING:\n",
    "    # This dictionary maps the original unique values from your 'status' column\n",
    "    # (now named 'label_original') to standardized sentiment categories.\n",
    "    # MAKE SURE THE KEYS HERE EXACTLY MATCH THE UNIQUE VALUES FROM YOUR 'status' COLUMN,\n",
    "    # INCLUDING CASE. If your 'status' column has 'normal' (lowercase), use 'normal' as key.\n",
    "    LABEL_MAPPING = {\n",
    "        'Depression': 'negative',\n",
    "        'Suicidal': 'negative',\n",
    "        'Anxiety': 'negative',\n",
    "        'Stress': 'negative',\n",
    "        'Bi-Polar': 'negative',\n",
    "        'Personality Disorder': 'negative',\n",
    "        'Normal': 'neutral'\n",
    "        # If your 'status' column values are, for example, all lowercase:\n",
    "        # 'depression': 'negative',\n",
    "        # 'suicidal': 'negative',\n",
    "        # 'anxiety': 'negative',\n",
    "        # 'stress': 'negative',\n",
    "        # 'bi-polar': 'negative',\n",
    "        # 'personality disorder': 'negative',\n",
    "        # 'normal': 'neutral'\n",
    "    }\n",
    "\n",
    "    # Filter out rows where 'label_original' is not in our LABEL_MAPPING\n",
    "    initial_rows = len(df)\n",
    "    df = df[df['label_original'].isin(LABEL_MAPPING.keys())].copy()\n",
    "    filtered_rows = len(df)\n",
    "    if initial_rows != filtered_rows:\n",
    "        print(f\"Warning: Filtered out {initial_rows - filtered_rows} rows due to unmapped 'label_original' values.\")\n",
    "        print(f\"Remaining rows: {filtered_rows} rows.\")\n",
    "    if df.empty:\n",
    "        print(\"\\nFATAL WARNING: DataFrame is EMPTY after filtering/mapping. This means no valid labels were found after filtering.\")\n",
    "        print(\"Please check your LABEL_MAPPING keys against the actual unique values in your 'status' column (`df['label_original'].unique()`).\")\n",
    "        df = None # Ensure df is None if it's empty to prevent downstream errors.\n",
    "\n",
    "if df is not None and not df.empty:\n",
    "    # Create the 'sentiment' column by mapping the 'label_original' values\n",
    "    df['sentiment'] = df['label_original'].map(LABEL_MAPPING)\n",
    "\n",
    "    # Text cleaning function\n",
    "    def preprocess_text(text):\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
    "        tokens = word_tokenize(text)\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "        return \" \".join(filtered_tokens)\n",
    "\n",
    "    df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "    print(\"\\nData after preprocessing and sentiment mapping:\")\n",
    "    print(df.head())\n",
    "    print(\"\\nSentiment distribution (after mapping):\")\n",
    "    print(df['sentiment'].value_counts())\n",
    "else:\n",
    "    print(\"Skipping preprocessing: DataFrame is empty or not loaded correctly due to previous errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59492df2",
   "metadata": {},
   "source": [
    "## 5. Feature Extraction (TF-IDF)\n",
    "    \n",
    "    We'll convert the text data into numerical features that a machine learning model can understand using TF-IDF (Term Frequency-Inverse Document Frequency).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b6aded",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "        X = df['cleaned_text']\n",
    "        y = df['sentiment']\n",
    "    \n",
    "        # Split data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "        # Initialize and fit TF-IDF Vectorizer\n",
    "        tfidf_vectorizer = TfidfVectorizer(max_features=5000) # Limit features to avoid sparsity/memory issues\n",
    "        X_train_vec = tfidf_vectorizer.fit_transform(X_train)\n",
    "        X_test_vec = tfidf_vectorizer.transform(X_test)\n",
    "    \n",
    "        print(\\\"TF-IDF Vectorizer fitted. Shape of training data:\\\", X_train_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970d6f89",
   "metadata": {},
   "source": [
    "## 6. Train the Sentiment Model (Logistic Regression)\n",
    "    \n",
    "    We'll use a Logistic Regression model for sentiment classification. It's a good baseline and often performs well for text classification tasks.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9dd9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "        # Initialize and train Logistic Regression model\n",
    "        sentiment_model = LogisticRegression(max_iter=1000, solver='liblinear') # 'liblinear' often good for small datasets\n",
    "        sentiment_model.fit(X_train_vec, y_train)\n",
    "        print(\"Model training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a58998",
   "metadata": {},
   "source": [
    "## 7. Evaluate the Model\n",
    "    \n",
    "    It's important to evaluate how well our model performs on unseen data.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b63863",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "        y_pred = sentiment_model.predict(X_test_vec)\n",
    "        print(\\\"\\\\nClassification Report:\\\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "    \n",
    "        print(\\\"\\\\nAccuracy Score:\\\", accuracy_score(y_test, y_pred))\n",
    "    \n",
    "        # Test with some custom sentences\n",
    "        test_sentences = [\n",
    "            \"I feel so sad and alone today, nothing is going right.\\\",\n",
    "            \"Today was a great day! I had so much fun with my friends.\\\",\n",
    "            \"I'm just doing homework, feeling okay.\\\",\n",
    "            \"Everything is terrible, I don't know what to do anymore.\\\",\n",
    "            \"I am so happy with my progress in school.\\\"\n",
    "    \n",
    "        print(\\\"\\\\nTesting with custom sentences:\\\")\n",
    "        for sentence in test_sentences:\n",
    "            cleaned_sentence = preprocess_text(sentence)\n",
    "            sentence_vec = tfidf_vectorizer.transform([cleaned_sentence])\n",
    "            predicted_sentiment = sentiment_model.predict(sentence_vec)[0]\n",
    "            print(f\\\"Sentence: '{sentence}' -> Predicted Sentiment: {predicted_sentiment}\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f61925",
   "metadata": {},
   "source": [
    "## 8. Save the Model and Vectorizer\n",
    "    \n",
    "    To use the trained model in our chatbot application, we need to save it along with the TF-IDF vectorizer. We'll save them in the `chatbot_project/` root directory.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62e5a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sentiment_model is not None and tfidf_vectorizer is not None:\n",
    "        joblib.dump(sentiment_model, '../sentiment_model.pkl')\n",
    "        joblib.dump(tfidf_vectorizer, '../tfidf_vectorizer.pkl')\n",
    "        print(\\\"Model and vectorizer saved successfully as sentiment_model.pkl and tfidf_vectorizer.pkl.\\\")\n",
    "    else:\\n\",\n",
    "        print(\\\"Model or vectorizer not trained/available, skipping save.\\\")\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c771b706",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "    You have successfully trained and saved a sentiment analysis model. The `sentiment_analysis.py` file in the main project directory will now load these saved files to predict sentiment, allowing the chatbot to respond more intelligently to user input."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
